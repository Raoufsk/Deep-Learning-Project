# -*- coding: utf-8 -*-
"""MLP_genomic_11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15J6izi4mJJe_vjeixXl5TgZmEB3RPglN
"""

import keras
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt

from keras import layers
from keras import regularizers
from keras.layers import Dropout
from keras.utils import np_utils
from sklearn import preprocessing
from keras.models import Sequential
from sklearn.utils import class_weight
from sklearn.impute import SimpleImputer
from keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import classification_report
from tensorflow.keras.optimizers import SGD, Adam
from keras.layers import Input, Dense, Activation
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/preprocessed_file_v0_filtered', 
                 compression = "zip", header = 0, index_col = 0, sep = "\t")

counts = df.groupby("CANCER_TYPE").agg({"CANCER_TYPE":"count"})
to_keep = counts[counts["CANCER_TYPE"] > 4500]
df = df[df.CANCER_TYPE.isin(to_keep.index)]

data = df.drop("CANCER_TYPE", axis = 1)
labels = df["CANCER_TYPE"]

print(labels.unique(), len(labels.unique()))

encoder = preprocessing.LabelEncoder()
classes = encoder.fit_transform(labels)

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
clf = ExtraTreesClassifier(n_estimators=50)
clf = clf.fit(data, classes)
m = SelectFromModel(clf, prefit=True)
data_new = m.transform(data)

x_train, x_test, y_train, y_test = train_test_split(
									data_new, classes, 
									test_size = 0.2, 
									random_state = 0) 

print(y_train.shape)
print(x_train.shape)
print(y_test.shape)
print(x_test.shape)

learning_rate = 0.01
opt = 'ADAM'
size_batch = 256
nb_epochs = 150

model = Sequential()	
	
model.add(	
	Dense(256, input_dim = x_train.shape[1], activation = 'relu'))

model.add(	
	Dense(64, activation = 'relu',
	kernel_regularizer = regularizers.l2(0.3)))
	
model.add(	
	Dense(len(labels.unique()), activation = 'softmax',
	kernel_regularizer = regularizers.l2(0.3)))
	
model.compile(optimizer = opt,	
	loss = 'sparse_categorical_crossentropy',	
	metrics = ["accuracy"])

model.summary()

es = EarlyStopping(
	monitor = 'val_accuracy', mode = 'max', 
	verbose = 1, patience = 30)

class_weights = class_weight.compute_class_weight(
	class_weight = "balanced", classes = np.unique(y_train), y = y_train)
class_weights_d = {l:c for l,c in zip(np.unique(y_train), class_weights)}

print(class_weights_d)

history = model.fit(
	x_train, y_train, 
	validation_data = (x_test, y_test), 
	batch_size = size_batch, 
	epochs = nb_epochs, 
	class_weight = class_weights_d,
	verbose = 1, callbacks = [es])

metrics = ['loss', 'accuracy']
for n, metric in enumerate(metrics):
	name = metric.replace("_"," ").capitalize()
	plt.subplot(2,2,n+1)
	plt.plot(history.epoch, history.history[metric],  label='Train')
	plt.plot(history.epoch, history.history['val_'+metric], linestyle="--", label='Val')
	plt.xlabel('Epoch')
	plt.ylabel(name)
	if metric == 'loss':
	plt.ylim([0, plt.ylim()[1]])
	elif metric == 'auc':
	plt.ylim([0.8,1])
	else:
	plt.ylim([0,1])

	plt.legend()

score = model.evaluate(
	x_test, y_test, 
	batch_size = size_batch, verbose = 1)

print("Scores on test set: loss=%s accuracy=%s" % tuple(score))

y_pred = model.predict(x_test)
pred = []
for i in range(len(y_pred)):
  pred.append(list(y_pred[i]).index(max(y_pred[i])))

c_m = confusion_matrix(y_test, pred)

f1, ax1 = plt.subplots(1,1, figsize=(20,20))
sns.heatmap(c_m, annot = True, cmap = "YlGnBu")
ax1.set_xlabel('Predicted labels')
ax1.set_ylabel('True labels')
ax1.set_title('Confusion Matrix')

c_r = classification_report(y_test, pred, output_dict = True)

cr = pd.DataFrame(c_r)
f2, ax2 = plt.subplots(1,1, figsize=(20,20))
ax2 = sns.heatmap(cr, annot = True, cmap = "YlGnBu")
ax2.set_title('Classification report')